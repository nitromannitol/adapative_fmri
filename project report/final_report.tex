\documentclass[12pt]{article}
\usepackage{fullpage,graphicx,psfrag,amsmath,amsfonts,verbatim}
\usepackage[small,bf]{caption}
\usepackage{hyperref}
\usepackage{amsthm}


\input defs.tex


\bibliographystyle{alpha}

\title{Discovering signals in fMRI data; 
a Bayesian nonparametric approach   \\ \large{Final project for STAT308}}
\author{Ahmed Bou-Rabee, Wanrong Zhu, Zheng Xu, Mo Zhou}

\begin{document}
\maketitle

\section{Introduction} 

In this paper we formulate and test a method which can be used to adaptively identify 
clusters of signals in functional magnetic resonance imaging (fMRI) data. 
Roughly, fMRI measures the change in brain blood flow associated 
with mental activity \cite{huettel2004functional}. The brain is divided into tiny blocks known as voxels, and the intensity 
of the blood flow over each voxel is recorded at evenly spaced time intervals while the subject is stimulated. For example, suppose researchers wanted to identify regions 
of the brain associated with hunger or craving. To aid in this, fMRI readings can be taken while hungry subjects are shown pictures of food. 

An advantage of using fMRI is that it's a noninvasive procedure.  Due to this, there are many publicly available datasets \cite{poldrack2013toward}.
However, analyzing fMRI data poses many statistical challenges, one of them being the multiple comparisons problem. 
 Any sampling procedure involves error.  Because there are typically thousands voxels, it's likely that individual voxels 
 may have high readings, but not be statistically significant. Identifying significant clusters (not just individual voxels) introduces an additional challenge. 
 
 In this paper, we explore a way of estimating significant clusters in fMRI using a Bayesian method.  The method and some theory is presented in the first 
 section. In the second section, we test the method on simulated data and real data.

\section{Method}



\subsection{Generative prior}
First some notation. 
Let $\mathcal{P} = [0,1]$ be our sample space and let $\mathcal{D}$ represent location-time space, (for example $\mathcal{D} = \reals^3 \times \reals^+$).
Suppose we have $n$ data points $(p_1, d_1), \ldots, (p_n, d_n)$ where $p_i \in \mathcal{P}$ and $d_i \in \mathcal{D}$. 
The p-value $p_i$ represents our belief in $d_i$ being a null feature. That is, our belief that the brain voxel at that point in time is not correlated 
with the stimulus. 

Signals in fMRI data typically tend to be sparse. That is, most of the $p_i$ are likely null features and so are drawn from a $\Unif(0,1)$ distribution. 
There is a small, but unknown, number of unknown signal clusters which are located spatially in $\mathcal{D}$.  
Each of the $p$-values within each cluster is drawn from a probability distribution on $\mathcal{P}$
which places most of its mass around $0$.  We can encode these approximate beliefs about fMRI data in the following generative prior. 
\begin{equation}
\begin{split}
\mbox{number of signal clusters: } & k \sim \mbox{Truncated Poisson}(\lambda,  1, k_{max}) \\
\mbox{signal centers: }&  c_j \sim \Unif(\mathcal{D}) \mbox{ for $j = 1, \ldots, k$} \\
\mbox{signal radius: }&  r_j \sim \mbox{Truncated Normal}(\mu,\sigma,r_{min},r_{max}) \mbox{ for $j = 1, \ldots k$} \\
\mbox{signal strength: }& \beta_j \sim \Unif(\beta_{min},\beta_{max}) \mbox{ for $j = 1, \ldots, k$} \\
\mbox{p-values in signal clusters: }& p_i \sim \Beta(\frac{1}{\beta_j}, \beta_j), \mbox{when $x_i$ is in cluster $j$} \\
\mbox{p-values not in signal clusters: }& p_i \sim \Unif(0,1). 
\end{split}
\end{equation}


For lack of time, we did not put priors on the hyper-parameters in the generative prior. Also, note that in this model, it is possible for clusters to overlap. This is an intentional choice as it allows for greater freedom in cluster shape. Furthermore, biologically, if each cluster corresponds to a role, it's plausible that regions of the brain play multiple roles, and hence
are in multiple clusters. 

\subsection{Inventing the chain}

Having established our prior, we sample from the posterior by using a method heavily inspired by the one in \cite{stephens2000bayesian}. 
In the aforementioned paper, Matthew Stephens constructs a Markov birth-death process to determine the number of unknown components in a mixture model. Our prior is not a mixture model, so we can't use his method directly. However, we can construct a birth-death Markov process similar to his 
which (assuming the generative prior) has stationary distribution given by the posterior likelihood of the data. 

Some notation first: let $\mathcal{C}$ denote the set of clusters (spheres parameterized by centers and radii) and let $\mathcal{B}$ denote the set of $\beta_i$ from which 
we parameterize the $\beta$ distributions. Then, the likelihood of a set of $n$ p-values, $p^n$ is given by
 \[
L(\mathcal{C}, \mathcal{B}) = P( p^n | \mathcal{C}, \mathcal{B}) = \prod_{\mbox{$p_{i_j}$ in cluster $j$}}  \frac{ p_{i_j}^{\frac{1}{\beta_j} - 1} (1 - p_{i_j})^{\beta_j-1}}{B(\frac{1}{\beta_j}. \beta_j)},
\]

To construct the chain,  we view each configuration of the model parameters as a point in an infinite dimensional parameter space. 
We construct a  walk over this infinite dimensional parameter space which converges to what is known as a marked point process. 
A marked point process is a formal way to denote a posterior distribution over an infinite state space. 
This is a probabilistic way to denote the posterior distribution. 
You can think of it as the posterior likelihood. We simulate real time using Gillespie algorithm.



To do: insert details of the method. 


\subsection{Proving it works}
I'll try to do this sometime in the next few days, maybe not before the presentation. 



\begin{theorem}
Given the prior in 2.2, the chain converges to a stationary distribution. 
\end{theorem}
\begin{proof} 
To do. Chain is irreducible. Need to prove that it satisfies detailed balance condition 
listed in end of paper. 
\end{proof}



\section{Discovering signals}

\subsection{Toy-data}
To do:  Discuss implementation details, like choice of prior and how this affects performance on the toy data. 


\subsection{Real data}
To do: Discuss where the dataset came from. 
To do: Also discuss implementation details and compare performance to the p-filter. This is partly done 

\section{Extensions}
Don't need to work with just p-values, can work with intensities directly by specifying appropriate priors for null distributions
and for signal distributions. 

Update the radius prior to allow for a vector in arbitrary dimensions instead of just one fixed. 

Put priors on the hyperparameters and maximize this priors using EM. That is uniform prior over hyperhyperparameters. 

%%%% i use gillespie algorithm to simulate continuous time chain 


\bibliography{bibliography}


\end{document}


